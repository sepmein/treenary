DONE:
1. Record the **URL** of the websites crawled. To avoid duplicate counts in the each word.
2. Some url is shorted, thus can't be reached by the crawler, should add the base url to them.
3. Separated crawler, server and web application.

TODO:
1. Put this onto digitalOcean.
2. Re-Porter-Stemmer.
3. Record the relationships between URL and words.
4. Provide a web client of the Treenary.
5. Record both the original word and the stem of each word.
6. Provide an iOS app of Treenary.
7. Memory Consumption will continuously rise up. Solve this.
8. 程序中断后，Crawler应该能从上次中断的地方继续。
9. 需要继续加强url过滤算法，现阶段较多无法访问的url。
